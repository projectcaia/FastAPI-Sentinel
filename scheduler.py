# scheduler.py  â€” Caia ìë™ ë£¨í”„ (KST ê¸°ì¤€, ENV ì„ê³„ 0.50 í†µì¼ + Router í˜¸í™˜íŒ)
# - ì„ê³„ì¹˜: ENV CAIA_IMPORTANCE_BASE (ê¸°ë³¸ 0.50)
# - function_router ìµœì‹ íŒê³¼ í˜¸í™˜: chat_completion ì˜ì¡´ ì œê±°, ë‚´ë¶€ ë˜í¼ë¡œ ëŒ€ì²´
# - finalize_session: routerì— ìµœê·¼ Rawë¥¼ messagesë¡œ êµ¬ì„±í•´ ì „ë‹¬
# - /ready + /health ê²Œì´íŠ¸ ìœ ì§€, ì¬ì‹œë„ ì„¸ì…˜ ìœ ì§€
# - ìŠ¤ì¼€ì¤„: UMA 00:10 â†’ Archive 00:30 â†’ Merge 01:00 â†’ Finalize 01:05 â†’ Train 01:10 â†’ Report 01:20
# - Legacy(ìœ ì‚°í™”): í† ìš”ì¼ 01:40

import os
import time
import json
import logging
import schedule
import requests
import traceback
from collections import defaultdict
from datetime import datetime, timedelta, timezone

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Router (ìµœì‹ íŒ)
from function_router import CaiaFunctionRouter

# ë©”ëª¨ë¦¬ ê³µìœ  ì‹±ê¸€í†¤
try:
    from memory_manager import get_shared_memory
except ImportError:
    # êµ¬ë²„ì „ í˜¸í™˜ í´ë°±
    from memory_manager import CaiaMemory as _CaiaMemory  # type: ignore
    _GLOBAL_MEMORY = None
    def get_shared_memory():
        global _GLOBAL_MEMORY
        if _GLOBAL_MEMORY is None:
            _GLOBAL_MEMORY = _CaiaMemory(session_id="caia-shared")  # type: ignore
        return _GLOBAL_MEMORY

from memory_archive import run_archive_job
from memory_train import run_train_job

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOG_DIR = "storage/logs"
os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger("scheduler")

# ENV ê¸°ë³¸ê°’ í†µì¼
CAIA_IMPORTANCE_BASE = float(os.getenv("CAIA_IMPORTANCE_BASE", "0.50"))
MERGE_WINDOW_HOURS   = int(os.getenv("CAIA_MERGE_WINDOW_HOURS", "24"))   # ì¼ì¼ ë³‘í•© ì°½
LEGACY_WINDOW_DAYS   = int(os.getenv("CAIA_LEGACY_WINDOW_DAYS", "7"))    # ìœ ì‚°í™” ê¸°ê°„
OPENAI_MODEL         = os.getenv("OPENAI_MODEL", "gpt-4.1")

# ë¶€íŒ… ì˜µì…˜
SCHED_RUN_ON_BOOT = os.getenv("SCHED_RUN_ON_BOOT", "0") == "1"
SCHED_RUN_NOW     = os.getenv("SCHED_RUN_NOW", "")  # ì˜ˆ: "invoke,snapshot"

KST = timezone(timedelta(hours=9))


def log_event(name: str, status: str, detail: str | None = None):
    now = datetime.now(KST).isoformat()
    os.makedirs(LOG_DIR, exist_ok=True)
    with open(os.path.join(LOG_DIR, f"{name}.log"), "a", encoding="utf-8") as f:
        f.write(f"[{status.upper()}] {now} {('- ' + detail) if detail else ''}\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _sanitize_env(s: str) -> str:
    if not s:
        return ""
    bad = {ord(c): None for c in "\t\r\n \u200b\u200c\u200d\ufeff"}
    return s.strip().translate(bad)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ URL Resolver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _resolve_base() -> str:
    """
    PUBLIC_BASE_URL / FUNCTION_CALLING_URL ì–´ëŠ ìª½ì´ë“  ë°›ì•„ì„œ
    ë² ì´ìŠ¤ URLë¡œ ì •ê·œí™” (ë ìŠ¬ë˜ì‹œ ì œê±°)
    """
    base = _sanitize_env(os.getenv("PUBLIC_BASE_URL") or os.getenv("FUNCTION_CALLING_URL", ""))
    return base.rstrip("/")

def _resolve_invoke_url() -> str:
    """
    ìµœì¢… /memory/invoke ì—”ë“œí¬ì¸íŠ¸ë¡œ ì •ê·œí™”
    """
    base = _resolve_base()
    if not base:
        return ""  # ë¹„ì–´ìˆìœ¼ë©´ í˜¸ì¶œ ìŠ¤í‚µ
    if base.endswith("/memory/invoke"):
        return base
    if base.endswith("/memory"):
        return f"{base}/invoke"
    return f"{base}/memory/invoke"


def _session_with_retries():
    s = requests.Session()
    r = Retry(
        total=3,
        backoff_factor=0.8,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=frozenset({"GET", "POST"}),
        raise_on_status=False,
    )
    s.mount("http://", HTTPAdapter(max_retries=r))
    s.mount("https://", HTTPAdapter(max_retries=r))
    return s


def _router():
    """ê³µìœ  ë©”ëª¨ë¦¬ì— ê²°í•©ëœ ë¼ìš°í„° ë°˜í™˜"""
    mem = get_shared_memory()
    return CaiaFunctionRouter(mem)


def _health_gate(base_url: str) -> bool:
    """
    /ready, /health í†µê³¼ ì‹œì—ë§Œ True.
    ì‹¤íŒ¨í•˜ë©´ False (ë„¤íŠ¸ì›Œí¬/ì•±ì£½ìŒ ë°©ì§€).
    """
    if not base_url:
        logger.warning("[Gate] PUBLIC_BASE_URL/FUNCTION_CALLING_URL ë¹„ì–´ìˆìŒ â†’ ì™¸ë¶€ í˜¸ì¶œ ìŠ¤í‚µ")
        return False

    ready_url = f"{base_url}/ready"
    health_url = f"{base_url}/health"

    try:
        with _session_with_retries() as s:
            r1 = s.get(ready_url, timeout=6)
            r2 = s.get(health_url, timeout=6)
        ok1 = r1.ok and ('"ready"' in r1.text or '"status":"ready"' in r1.text)
        ok2 = r2.ok and '"qdrant":"ok"' in r2.text
        if not ok1 or not ok2:
            logger.warning("[Gate] í—¬ìŠ¤ ë¶ˆí†µ ready=%s health=%s text1=%s text2=%s",
                           r1.status_code, r2.status_code, (r1.text[:120] if r1.text else ""), (r2.text[:120] if r2.text else ""))
        return bool(ok1 and ok2)
    except Exception as e:
        logger.error("[Gate] í—¬ìŠ¤ í˜¸ì¶œ ì˜ˆì™¸: %s", e)
        return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI ìš”ì•½ ë˜í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _openai_client():
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        return None
    try:
        from openai import OpenAI
        return OpenAI(api_key=key)
    except Exception:
        return None

def _chat_completion(prompt: str, *, max_tokens: int = 700, temperature: float = 0.2) -> str:
    client = _openai_client()
    if not client:
        return ""
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ëµì„ ê¸°ì–µÂ·ìœ ì‚°í™”í•˜ëŠ” íŒë‹¨ì Caiaì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤í–‰ê°€ëŠ¥í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        logger.warning("[LLM] chat_completion ì˜ˆì™¸: %s", e)
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  invoke ë£¨í”„ (ë³´ì¡°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_invoke():
    """ì™¸ë¶€ ì—”ë“œí¬ì¸íŠ¸ /memory/invoke í˜¸ì¶œ â†’ ìš”ì•½ ì´ˆì•ˆ ì €ì¥(ë³´ì¡°ìš©)."""
    try:
        base = _resolve_base()
        target_url = _resolve_invoke_url()
        logger.info("[Invoke] base=%s target=%s", base or "(empty)", target_url or "(empty)")

        if not target_url or not _health_gate(base):
            log_event("invoke", "skip", "health-gate-fail-or-empty-base")
            logger.warning("[Invoke] ìŠ¤í‚µë¨(ë² ì´ìŠ¤ ë¹„ì—ˆê±°ë‚˜ í—¬ìŠ¤ ë¶ˆí†µ)")
            return

        payload = {
            "messages": [
                {"type": "system", "content": f"[Caia ìë™ íŒë‹¨ ë£¨í”„] {datetime.now(KST).isoformat()}"}
            ]
        }

        with _session_with_retries() as s:
            res = s.post(target_url, json=payload, timeout=20, headers={"Content-Type": "application/json"})
        if 200 <= res.status_code < 300:
            log_event("invoke", "success", f"HTTP {res.status_code}")
            logger.info("[Invoke] âœ… %s", (res.text or "")[:300])
        else:
            log_event("invoke", "fail", f"HTTP {res.status_code} {res.text[:180] if res.text else ''}")
            logger.error("[Invoke] âŒ HTTP %s %s", res.status_code, (res.text or "")[:300])

    except Exception as e:
        log_event("invoke", "fail", traceback.format_exc())
        logger.error("[Invoke] ì˜ˆì™¸ âŒ %s", e)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡ ìŠ¤ëƒ…ìƒ·/ì•„ì¹´ì´ë¸Œ/íŠ¸ë ˆì¸/ë¦¬íŠ¸ë¦¬ë¸Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_snapshot():
    try:
        r = _router()
        # UMA ì…ë ¥ ìŠ¤ëƒ…ìƒ·: ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ Rawì— ë‚¨ê¹€ (ìš”ì•½ì€ ìƒëµ)
        r.analyze_and_route(
            messages=[{"type": "system", "content": f"âœ… UMA snapshot {datetime.now(KST).isoformat()}"}],
            topic="UMA",
            do_micro_digest=False
        )
        r.memory.save({"type": "system", "content": "âœ… snapshot", "category": "ìŠ¤ì¼€ì¤„ëŸ¬"})
        log_event("snapshot", "success")
        logger.info("[Snapshot] ì™„ë£Œ âœ…")
    except Exception as e:
        log_event("snapshot", "fail", traceback.format_exc())
        logger.error("[Snapshot] ì˜ˆì™¸ âŒ %s", e)


def run_archive():
    try:
        res = run_archive_job()
        log_event("archive", "success", f"archived={res.get('archived')} path={res.get('path')}")
        logger.info("[Archive] ì™„ë£Œ âœ… archived=%s file=%s", res.get("archived"), res.get("path"))
    except Exception as e:
        log_event("archive", "fail", traceback.format_exc())
        logger.error("[Archive] ì˜ˆì™¸ âŒ %s", e)


def run_train():
    try:
        res = run_train_job()
        log_event("train", "success", f"trained={res.get('trained')}")
        logger.info("[Train] ì™„ë£Œ âœ… trained=%s topics=%s", res.get("trained"), res.get("topics"))
    except Exception as e:
        log_event("train", "fail", traceback.format_exc())
        logger.error("[Train] ì˜ˆì™¸ âŒ %s", e)


def run_retrieve_digest():
    try:
        r = _router()
        keywords = ["ì½œë§¤ë„", "Reflex", "ì•ŒíŒŒ ì „ëµ ì‹¤íŒ¨"]
        digest = [{"q": kw, "results": r.retrieve(kw)} for kw in keywords]
        os.makedirs("storage", exist_ok=True)
        path = os.path.join("storage", f"digest_{datetime.utcnow().isoformat()}.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(digest, f, ensure_ascii=False, indent=2)
        r.memory.save({"type": "system", "content": "âœ… retrieve_digest", "category": "ìŠ¤ì¼€ì¤„ëŸ¬"})
        log_event("retrieve_digest", "success")
        logger.info("[RetrieveDigest] ì €ì¥ ì™„ë£Œ âœ… %s", path)
    except Exception as e:
        log_event("retrieve_digest", "fail", traceback.format_exc())
        logger.error("[RetrieveDigest] ì˜ˆì™¸ âŒ %s", e)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¢ ë³‘í•© ìš”ì•½ (í•µì‹¬, L1 ìš°ì„ ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _priority_score(item: dict) -> float:
    """L1(ìˆ˜ë™/ê²°ì •/ì´ˆì•ˆ) ìš°ì„  ê°€ì¤‘ì¹˜ + importance"""
    base = float(item.get("importance", 0.0))
    t = (item.get("type") or "").lower()
    tags = set(item.get("tags") or [])
    bonus = 0.0
    if "manual-feedback" in tags or t == "manual":
        bonus += 0.25
    if "implicit-decision" in tags or t == "decision":
        bonus += 0.20
    if t == "digest_draft":
        bonus += 0.10
    return min(1.0, base + bonus)

def run_merge_digest():
    """
    ì§€ë‚œ MERGE_WINDOW_HOURS Raw ì¤‘ ì¤‘ìš”ë„ ì„ê³„ ì´ìƒ + L1 ìš°ì„  í•­ëª©ì„ topicë³„ë¡œ ë¬¶ì–´
    700~900ì Digest ìƒì„± â†’ Qdrantì— level='long'ìœ¼ë¡œ ì €ì¥.
    """
    try:
        r = _router()
        memory = r.memory

        if not getattr(memory, "vectorstore", None):
            log_event("merge_digest", "success", "vectorstore-missing")
            logger.warning("[MergeDigest] vectorstore ë¯¸íƒ‘ì¬ â†’ ì„ë² ë”©/ì €ì¥ ìŠ¤í‚µ")
            return

        since_ts = (datetime.utcnow() - timedelta(hours=MERGE_WINDOW_HOURS)).isoformat()
        items = memory.list_raw(min_importance=CAIA_IMPORTANCE_BASE, since_ts=since_ts)

        if not items:
            log_event("merge_digest", "success", "no items")
            logger.info("[MergeDigest] ì¤‘ìš”ë„ ê¸°ì¤€ ì¶©ì¡± í•­ëª© ì—†ìŒ")
            return

        # í† í”½ ê·¸ë£¹ + ìš°ì„ ìˆœìœ„ ì •ë ¬(L1 ê°€ì¤‘ì¹˜)
        groups: dict[str, list[dict]] = defaultdict(list)
        for it in items:
            groups[it.get("topic", "ì¼ë°˜")].append(it)

        created = 0
        for topic, arr in groups.items():
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œë§Œ ì‚¬ìš© (ê³¼ë‹¤ ê¸¸ì´ ë°©ì§€)
            arr_sorted = sorted(arr, key=_priority_score, reverse=True)
            top = arr_sorted[:20]  # topicë‹¹ ìµœëŒ€ 20 bullet

            bullets = "\n".join(f"- {x.get('content','')[:500]}" for x in top)
            prompt = f"""ë‹¤ìŒ í•µì‹¬ì„ ì¤‘ë³µ ì—†ì´ 700~900ì í•œêµ­ì–´ë¡œ ìš”ì•½í•´.
ëª©ì : Caia ì¥ê¸° ê¸°ì–µ(Digest). ìš°ì„ ìˆœìœ„: ìˆ˜ë™í”¼ë“œë°±/ê²°ì •/ì´ˆì•ˆ > ì¼ë°˜.
í˜•ì‹: ê²°ë¡ â†’ê·¼ê±°â†’ë‹¤ìŒ í–‰ë™. ê³¼ì¥ ê¸ˆì§€.
---
{bullets}
"""
            digest = _chat_completion(prompt, max_tokens=700, temperature=0.2) or ""
            if not digest.strip():
                continue

            source_span = (top[0].get("timestamp"), top[-1].get("timestamp"))
            memory.add_vector_memory(
                digest,
                metadata={
                    "level": "long",
                    "topic": topic,
                    "tags": ["merge", "l1-priority"],
                    "source_span": list(source_span),
                    "ts": datetime.utcnow().isoformat(),
                },
            )
            created += 1

        log_event("merge_digest", "success", f"created={created}")
        logger.info("[MergeDigest] ìƒì„± ê±´ìˆ˜ âœ… %d", created)
    except Exception as e:
        log_event("merge_digest", "fail", traceback.format_exc())
        logger.error("[MergeDigest] ì˜ˆì™¸ âŒ %s", e)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘£ ìœ ì‚°í™” (ì£¼ 1íšŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_legacy_promote():
    """
    ìµœê·¼ 7ì¼ Raw(ì¤‘ìš”ë„ ê¸°ì¤€ ì¶©ì¡±)ë¥¼ OBAR(ì›ì¸-í–‰ë™-ê²°ê³¼-êµí›ˆ)ë¡œ 1~2p ì„œìˆ .
    Qdrantì— level='legacy'ë¡œ ì €ì¥.
    """
    try:
        r = _router()
        memory = r.memory

        if not getattr(memory, "vectorstore", None):
            log_event("legacy_promote", "success", "vectorstore-missing")
            logger.warning("[Legacy] vectorstore ë¯¸íƒ‘ì¬ â†’ ì„ë² ë”©/ì €ì¥ ìŠ¤í‚µ")
            return

        since_ts = (datetime.utcnow() - timedelta(days=LEGACY_WINDOW_DAYS)).isoformat()
        items = memory.list_raw(min_importance=CAIA_IMPORTANCE_BASE, since_ts=since_ts)

        if not items:
            log_event("legacy_promote", "success", "no items")
            logger.info("[Legacy] ìµœê·¼ 7ì¼ ì¤‘ìš” í•­ëª© ì—†ìŒ")
            return

        groups = defaultdict(list)
        for it in items:
            groups[it.get("topic", "ì¼ë°˜")].append(it)

        created = 0
        for topic, arr in groups.items():
            # OBAR ì„œìˆ 
            body = "\n".join(f"- {x.get('content','')[:500]}" for x in arr)
            prompt = f"""ì§€ë‚œ 7ì¼ê°„ '{topic}' ê´€ë ¨ í•­ëª©ì„ OBARë¡œ 1~2í˜ì´ì§€ ë¶„ëŸ‰ í•œêµ­ì–´ ì„œìˆ ë¡œ ì •ë¦¬í•´.
í˜•ì‹:
[ì›ì¸] ...
[í–‰ë™] ...
[ê²°ê³¼] ...
[êµí›ˆ] ...
êµ°ë”ë”ê¸° ì—†ì´ ì‹¤ë¬´ ë¬¸ì„œ í†¤ìœ¼ë¡œ.
---
{body}
"""
            legacy_text = _chat_completion(prompt, max_tokens=1200, temperature=0.2) or ""
            if not legacy_text.strip():
                continue

            iso = datetime.utcnow().isocalendar()
            week_id = f"{iso[0]}-W{iso[1]:02d}"

            memory.add_vector_memory(
                legacy_text,
                metadata={
                    "level": "legacy",
                    "topic": topic,
                    "week_id": week_id,
                    "ts": datetime.utcnow().isoformat(),
                },
            )
            created += 1

        log_event("legacy_promote", "success", f"created={created}")
        logger.info("[Legacy] ìŠ¹ê²© ê±´ìˆ˜ âœ… %d", created)
    except Exception as e:
        log_event("legacy_promote", "fail", traceback.format_exc())
        logger.error("[Legacy] ì˜ˆì™¸ âŒ %s", e)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¤ ì„¸ì…˜ ë§ˆê°(L2) ìë™í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_finalize_session():
    """
    ìµœê·¼ MERGE_WINDOW_HOURS ì°½ì„ L2(Session-Digest)ë¡œ ìŠ¹ê²©.
    function_router ìµœì‹ íŒì˜ finalize_session(messages, topic) ì„œëª…ê³¼ í˜¸í™˜ë˜ë„ë¡,
    ìµœê·¼ Rawë¥¼ messages ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±í•´ ì „ë‹¬í•œë‹¤.
    """
    try:
        r = _router()
        memory = r.memory
        since_ts = (datetime.utcnow() - timedelta(hours=MERGE_WINDOW_HOURS)).isoformat()
        items = memory.list_raw(min_importance=CAIA_IMPORTANCE_BASE, since_ts=since_ts)

        if not items:
            log_event("finalize_session", "success", "no items")
            logger.info("[FinalizeSession] ìµœê·¼ ì°½ ë‚´ í•­ëª© ì—†ìŒ")
            return

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸í•˜ê³  ì‚¬ëŒ/AI ë°œí™”ë§Œ messagesë¡œ êµ¬ì„±
        msgs = []
        for it in items:
            t = (it.get("type") or "human").lower()
            if t == "system":
                continue
            c = (it.get("content") or "").strip()
            if not c:
                continue
            msgs.append({"type": t, "content": c})

        res = r.finalize_session(messages=msgs, topic=None)
        created = res.get("created", 0) if isinstance(res, dict) else 0
        log_event("finalize_session", "success", f"created={created}")
        logger.info("[FinalizeSession] L2 ìƒì„± ê±´ìˆ˜ âœ… %s", created)
    except Exception as e:
        log_event("finalize_session", "fail", traceback.format_exc())
        logger.error("[FinalizeSession] ì˜ˆì™¸ âŒ %s", e)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¥ ìŠ¤ì¼€ì¤„ëŸ¬ ë¶€íŠ¸ìŠ¤íŠ¸ë© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def start_scheduler():
    logger.info("ğŸŸ¢ [Caia Scheduler] ìŠ¤ì¼€ì¤„ëŸ¬ ë£¨í”„ ì‹œì‘ë¨ (TZ=%s)", os.getenv("TZ", "system-default"))

    # ì¤‘ë³µ ë°©ì§€
    schedule.clear('caia')

    # ë¬¸ì„œ ê¸°ì¤€(KST): UMA 00:10 â†’ Archive 00:30 â†’ **Merge 01:00** â†’ Finalize 01:05 â†’ Train 01:10 â†’ Report 01:20
    schedule.every().day.at("00:10").do(run_snapshot).tag('caia')           # UMA ì…ë ¥ ìŠ¤ëƒ…ìƒ·
    schedule.every().day.at("00:30").do(run_archive).tag('caia')            # ì•„ì¹´ì´ë¸Œ
    schedule.every().day.at("01:00").do(run_merge_digest).tag('caia')       # âœ… í•µì‹¬: L1ìš°ì„  Digest ìƒì„±/ì„ë² ë”©
    schedule.every().day.at("01:05").do(run_finalize_session).tag('caia')   # âœ… L2 ì„¸ì…˜ ë§ˆê° ìš”ì•½
    schedule.every().day.at("01:10").do(run_train).tag('caia')              # ë³´ê°•(ìš”ì•½ ì„ë² ë”©)
    schedule.every().day.at("01:20").do(run_retrieve_digest).tag('caia')    # ë¦¬í¬íŠ¸(ì„ íƒ)

    # ìœ ì‚°í™”: **í† ìš”ì¼ ìƒˆë²½ë§Œ 01:40** ê³ ì • (ì£¼1íšŒ)
    schedule.every().saturday.at("01:40").do(run_legacy_promote).tag('caia')

    # ë‹¤ìŒ ì‹¤í–‰ ì‹œê° ë¡œê·¸
    for job in schedule.get_jobs('caia'):
        logger.info("â° ìŠ¤ì¼€ì¤„ ë“±ë¡: %-18s â†’ next_run=%s", job.job_func.__name__, job.next_run)

    # ë¶€íŒ…ì ê²€: ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰(ì˜µì…˜)
    if SCHED_RUN_ON_BOOT:
        logger.info("ğŸš€ ë¶€íŒ…í›„ ì¦‰ì‹œ ì ê²€(SCHED_RUN_ON_BOOT=1)")
        try:
            run_invoke()
            run_snapshot()
        except Exception as e:
            logger.error("ë¶€íŒ…ì ê²€ ì˜ˆì™¸: %s", e)

    if SCHED_RUN_NOW:
        # ì˜ˆ: SCHED_RUN_NOW="invoke,snapshot,merge_digest"
        now_jobs = [j.strip() for j in SCHED_RUN_NOW.split(",") if j.strip()]
        logger.info("âš¡ ìˆ˜ë™ ì¦‰ì‹œ ì‹¤í–‰(SCHED_RUN_NOW): %s", now_jobs)
        for name in now_jobs:
            try:
                globals()[f"run_{name}"]()
            except KeyError:
                logger.warning("ì•Œ ìˆ˜ ì—†ëŠ” ì¦‰ì‹œ ì‹¤í–‰ ì¡: %s", name)
            except Exception as e:
                logger.error("ì¦‰ì‹œ ì‹¤í–‰ ì˜ˆì™¸(%s): %s", name, e)

    while True:
        schedule.run_pending()
        time.sleep(15)


if __name__ == "__main__":
    start_scheduler()
