# server.py â€” Caia Memory Server (Option A: Web + Scheduler in one process)
# EMBED_DIM import ì œê±°íŒ (memory_managerì— EMBED_DIMê°€ ì—†ì„ ë•Œë„ ì•ˆì „)

from __future__ import annotations
import os
import sys
import time
import threading
from typing import Any, Dict, List, Optional

# (ë¡œì»¬ ì‹¤í–‰ ì‹œ .env ìë™ ë¡œë“œ â€” í”„ë¡œë•ì…˜ì— ì˜í–¥ ì—†ìŒ)
try:
    from dotenv import load_dotenv  # type: ignore
    load_dotenv()
except Exception:
    pass

from fastapi import FastAPI
from fastapi.responses import JSONResponse, PlainTextResponse
from pydantic import BaseModel, Field

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ëª¨ë“ˆ ì˜ì¡´ â€” EMBED_DIM import ì‚­ì œ (ì—¬ê¸°ì„œ ê³„ì‚°/í‘œì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from memory_manager import (
        get_shared_memory,          # ë™ì¼ í”„ë¡œì„¸ìŠ¤ ê³µìœ  ë©”ëª¨ë¦¬ í•¸ë“¤ëŸ¬
        ensure_qdrant_collection,   # ì»¬ë ‰ì…˜/ì¸ë±ìŠ¤ ë³´ì¥
        QDRANT_COLLECTION,
        make_qdrant,
    )
except Exception as e:  # noqa: BLE001
    print(f"âŒ memory_manager import ì‹¤íŒ¨: {type(e).__name__}: {e}", file=sys.stderr)
    raise

# (ì„ íƒ) OpenAI SDK â€” /memory/invoke ë³´ì¡° ìš”ì•½ì—ë§Œ ì‚¬ìš©
try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore

APP_START_TS = time.time()
APP_VERSION  = "2025-08-18"
app = FastAPI(title="Caia Memory Server", version=APP_VERSION)

# ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ê¸°ë™ ì˜µì…˜
SCHED_AUTOSTART = os.getenv("SCHED_AUTOSTART", "1")
_sched_started_flag = threading.Event()

# ì¤‘ìš” ENV (ì§„ë‹¨ ë…¸ì¶œìš©)
CAIA_IMPORTANCE_BASE = os.getenv("CAIA_IMPORTANCE_BASE", "0.5")
RAW_BACKUP_PATH      = os.getenv("RAW_BACKUP_PATH", "").strip()
PUBLIC_BASE_URL      = os.getenv("PUBLIC_BASE_URL", "")
FUNCTION_CALLING_URL = os.getenv("FUNCTION_CALLING_URL", "")

def _embed_dim_probe() -> Optional[int]:
    """
    ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ embed_dimì„ ì–»ê³ , ì—†ìœ¼ë©´ ENV EMBEDDING_DIM(ê¸°ë³¸ 1536)ë¡œ ëŒ€ì²´.
    """
    try:
        mem = get_shared_memory()
        dim = getattr(mem, "embed_dim", None)
        if isinstance(dim, int) and dim > 0:
            return dim
    except Exception:
        pass
    try:
        return int(os.getenv("EMBEDDING_DIM", "1536"))
    except Exception:
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Scheduler ë¶€íŠ¸ìŠ¤íŠ¸ë© (ì˜µì…˜ A í•µì‹¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _start_scheduler_background() -> None:
    if _sched_started_flag.is_set():
        return
    try:
        from scheduler import start_scheduler  # ì§€ì—° ì„í¬íŠ¸(ìˆœí™˜ì°¸ì¡° íšŒí”¼)
    except Exception as e:  # noqa: BLE001
        print(f"âš ï¸ scheduler ì„í¬íŠ¸ ì‹¤íŒ¨: {type(e).__name__}: {e}", file=sys.stderr)
        return

    def _run():
        try:
            start_scheduler()
        except Exception as e:  # noqa: BLE001
            print(f"âŒ scheduler ì¢…ë£Œ: {type(e).__name__}: {e}", file=sys.stderr)

    t = threading.Thread(target=_run, daemon=True, name="caia-scheduler")
    t.start()
    _sched_started_flag.set()
    print("ğŸ•’ Scheduler thread started (daemon)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI ìŠ¤íƒ€íŠ¸ì—… í›…: Qdrant ë³´ì¥ + ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë™
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.on_event("startup")
def on_startup() -> None:
    # 1) Qdrant ì»¬ë ‰ì…˜/ì¸ë±ìŠ¤ ë³´ì¥ (embed dimì€ memory_manager ë‚´ë¶€ì—ì„œ ê²°ì •)
    try:
        qc = make_qdrant()
        ensure_qdrant_collection(qc, QDRANT_COLLECTION)
        print(f"âœ… Qdrant ready: collection={QDRANT_COLLECTION}")
    except Exception as e:  # noqa: BLE001
        print(f"âš ï¸ Qdrant ensure ì‹¤íŒ¨: {e}", file=sys.stderr)

    # 2) ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ê¸°ë™
    if SCHED_AUTOSTART.lower() in ("1", "true", "yes", "y"):
        _start_scheduler_background()
    else:
        print("â„¹ï¸ SCHED_AUTOSTART=0 â†’ scheduler autostart disabled")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒíƒœ/ì§„ë‹¨ ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/", response_class=PlainTextResponse)
def root() -> str:
    return "Caia Memory Server"

@app.get("/ready")
def ready() -> JSONResponse:
    return JSONResponse({
        "status": "ready",
        "uptime_sec": round(time.time() - APP_START_TS, 3),
        "scheduler_started": _sched_started_flag.is_set()
    })

@app.get("/health")
def health() -> JSONResponse:
    ok = True
    detail: Dict[str, Any] = {}
    try:
        qc = make_qdrant()
        _ = qc.get_collections()
        detail["qdrant"] = "ok"
    except Exception as e:  # noqa: BLE001
        ok = False
        detail["qdrant"] = f"error: {type(e).__name__}: {e}"
    return JSONResponse({"status": "ok" if ok else "error", "detail": detail})

@app.get("/diag/env")
def diag_env() -> JSONResponse:
    return JSONResponse({
        "version": APP_VERSION,
        "collection": QDRANT_COLLECTION,
        "embed_dim": _embed_dim_probe(),           # ğŸ”¹ ì—¬ê¸°ì„œ ì•ˆì „í•˜ê²Œ í‘œì‹œ
        "scheduler_started": _sched_started_flag.is_set(),
        "env": {
            "CAIA_IMPORTANCE_BASE": CAIA_IMPORTANCE_BASE,
            "RAW_BACKUP_PATH_set": bool(RAW_BACKUP_PATH),
            "PUBLIC_BASE_URL_set": bool(PUBLIC_BASE_URL),
            "FUNCTION_CALLING_URL_set": bool(FUNCTION_CALLING_URL),
            "QDRANT_URL_set": bool(os.getenv("QDRANT_URL")),
            "QDRANT_API_KEY_set": bool(os.getenv("QDRANT_API_KEY")),
            "OPENAI_API_KEY_set": bool(os.getenv("OPENAI_API_KEY")),
            "SCHED_AUTOSTART": SCHED_AUTOSTART,
        },
    })

class DiagIn(BaseModel):
    collection: Optional[str] = None
    ensure_indexes: bool = True

@app.post("/diag/qdrant")
def diag_qdrant(body: DiagIn) -> JSONResponse:
    coll = body.collection or QDRANT_COLLECTION
    try:
        qc = make_qdrant()
        created = ensure_qdrant_collection(qc, coll, ensure_indexes=body.ensure_indexes)
        return JSONResponse({"status": "ok", "collection": coll, "created": created, "embed_dim": _embed_dim_probe()})
    except Exception as e:  # noqa: BLE001
        return JSONResponse({"status": "error", "error": str(e)}, status_code=500)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ëª¨ë¦¬ API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Message(BaseModel):
    type: str = Field(default="human")  # human/ai/system
    content: str
    topic: Optional[List[str]] = None
    level: Optional[str] = "L2"

class EchoIn(BaseModel):
    messages: List[Message]

@app.post("/memory/echo")
def memory_echo(body: EchoIn) -> JSONResponse:
    mem = get_shared_memory()
    out = []
    for m in body.messages:
        data = {
            "type": (m.type or "human").lower(),
            "content": m.content or "",
            "topic": (m.topic or []),
            "level": (m.level or "L2"),
            "timestamp": mem.now_iso(),
        }
        mem.echo(data)
        out.append({
            "type": data["type"],
            "content": data["content"],
            "topic": data["topic"],
            "level": data["level"],
        })
    return JSONResponse(out)

class RetrieveIn(BaseModel):
    query: str
    top_k: int = 5
    topics_any: Optional[List[str]] = None

@app.post("/memory/retrieve")
def memory_retrieve(body: RetrieveIn) -> JSONResponse:
    mem = get_shared_memory()
    hits = mem.vector_search(body.query, top_k=max(1, body.top_k), topics_any=body.topics_any)
    recalled = [h.get("text", "") for h in hits if h.get("text")]
    return JSONResponse({"recalled": recalled, "hits": hits})

class InvokeIn(BaseModel):
    messages: Optional[List[Message]] = None

@app.post("/memory/invoke")
def memory_invoke(body: InvokeIn) -> JSONResponse:
    mem = get_shared_memory()

    # ê°„ì´ ìš”ì•½(ëª¨ë¸ ì—†ì´ ë¼ì¸ ì••ì¶•)
    lines: List[str] = []
    for m in (body.messages or [])[-10:]:
        role = (m.type or "human").strip()
        content = (m.content or "").strip().replace("\n", " ")
        if content:
            lines.append(f"- ({role}) {content[:200]}")
        if len(lines) >= 5:
            break
    summary = "\n".join(lines)

    if summary.strip():
        try:
            mem.add_vector_memory(summary, metadata={
                "level": "L2",
                "topic": "summary",
                "tags": ["invoke"],
                "ts": mem.now_iso(),
            })
        except Exception:
            pass

    return JSONResponse({"status": "invoked", "ok": True, "summary_len": len(summary)})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œì»¬ ì§ì ‘ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    if SCHED_AUTOSTART.lower() in ("1", "true", "yes", "y"):
        _start_scheduler_background()
    uvicorn.run("server:app", host="0.0.0.0", port=int(os.getenv("PORT", "8000")), reload=False, log_level="info")
